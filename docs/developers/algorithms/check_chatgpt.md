## <a name="header"></a><a name="content"></a><a name="scalarfield.fft_space-medium-severity"></a>1. ScalarField.fft\_space – *Medium Severity*
**Finding:** The ScalarField.fft\_space method correctly converts spatial data to k-space using an angular wavenumber definition (k = 2π \* fftfreq(...)), ensuring units are handled (e.g. if spatial axis in meters, k-axis in 1/m)[\[1\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fields/scalar.py#L538-L546). It also flips the sign of the k-axis if the original spatial axis was in descending order[\[2\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fields/scalar.py#L540-L548), preserving physical orientation. However, unlike fft\_time() (which normalizes by 1/N and doubles one-sided amplitudes)[\[3\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fields/scalar.py#L284-L292), fft\_space performs an unnormalized N-point FFT (two-sided) with no explicit amplitude scaling. This discrepancy means that after fft\_space, the spectrum’s amplitude is N times larger than the spatial-domain amplitude (since SciPy’s forward FFT is unnormalized). The code relies on the inverse FFT (which divides by N) to restore amplitude later, but during analysis the raw k-space field’s amplitude is not normalized. This inconsistent normalization between time-FFT and space-FFT is noted as a potential source of user confusion[\[4\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/docs/developers/plans/fft_documentation_plan_20260124_1350.md#L40-L45) but not a bug per se. Overall, unit consistency is maintained via \_validate\_domain\_units (requiring time axes in seconds, k-axes in 1/length, etc.) and the wavenumber axes are correctly labeled (e.g. “x” → “kx”)[\[1\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fields/scalar.py#L538-L546)[\[5\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fields/scalar.py#L548-L555).

**Recommendation:** Document clearly that fft\_space returns the *full* two-sided spatial frequency spectrum without 1/N normalization, whereas fft\_time produces a normalized one-sided spectrum. This distinction should be emphasized in user guides (as planned in the FFT conventions documentation[\[6\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/docs/developers/plans/fft_documentation_plan_20260124_1350.md#L18-L25)[\[4\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/docs/developers/plans/fft_documentation_plan_20260124_1350.md#L40-L45)) to avoid misinterpretation of amplitude. If amplitude preservation in spatial FFT is important, consider providing an option or helper for normalization (e.g. dividing by √N or similar) or explicitly state that Parseval’s theorem holds only when comparing total energy (power) rather than raw amplitude in k-space. Since the current implementation is physically consistent (no unit errors)[\[1\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fields/scalar.py#L538-L546), the main action item is improved documentation. Additionally, tests could verify that an inverse ifft\_space recovers the original field (ensuring the 2π scaling and sign flips don’t introduce bias), which would instill confidence in the numerical stability of the forward/inverse pair.
## <a name="x50c763c5a6d1867baf84824c3afabdabfa51764"></a>2. ResponseFunctionAnalysis & \_fft\_transient – *Low Severity*
**Finding:** The stepped-sine **step detection logic** in ResponseFunctionAnalysis is sound and methodical. It uses a high-resolution spectrogram (window = 1s by default) to track the peak frequency and amplitude in the witness channel over time[\[7\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L204-L212)[\[8\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L228-L237). For each time bin, it flags a segment as “injection on” when the peak SNR exceeds a threshold and stays at roughly constant frequency (within 1 Hz tolerance by default). The code accumulates continuous loud segments and trims their edges by 1 s before output[\[9\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L228-L237)[\[10\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L236-L244). This effectively identifies steady-frequency intervals for each injection step. It also ensures segments meet a minimum duration (default 5 s) before accepting them[\[11\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L234-L242)[\[12\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L256-L263). This logic is appropriate for avoiding false positives due to short fluctuations. One minor consideration is that the threshold uses the **global median** of the spectrogram for SNR[\[13\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L213-L221); if background noise is non-stationary, some quiet steps might be missed or noisy segments mistakenly flagged. However, since snr\_threshold is configurable and the procedure is conservative (requires consistent frequency and duration), the risk is low. On the FFT side, the custom \_fft\_transient mode preserves signal amplitude for transient signals by applying the same normalization as GWpy’s FFT: dividing by N and doubling the positive frequency components[\[14\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/_spectral_fourier.py#L208-L217)[\[15\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/_spectral_fourier.py#L214-L221). This means a pure sinusoid injection yields a spectral peak equal to its time-domain amplitude, which is crucial for accurate coupling factor calculation. The amplitude preservation is evident: the code explicitly divides by target\_nfft and multiplies non-DC bins by 2[\[16\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/_spectral_fourier.py#L208-L216), matching the logic in TimeSeries.fft()[\[3\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fields/scalar.py#L284-L292). Therefore, the computed Amplitude Spectral Densities (ASDs) for target and background in each segment retain correct scaling (the peak at the injection frequency reflects the true signal amplitude). The subsequent coupling factor calculation uses these ASDs (squared differences)[\[17\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L431-L438), and the correctness of amplitude scaling ensures that a significant excess in the target ASD at the injection frequency is accurately captured.

**Recommendation:** No major implementation changes are needed, as both the detection algorithm and FFT handling are appropriate. For **step detection**, one could consider exposing freq\_tolerance (currently fixed at 1 Hz) as a parameter if users need to detect very close frequency changes, but the default is reasonable for typical stepped-sine injections. It would also be wise to mention in documentation that the first one or two seconds of each detected segment are trimmed to avoid edge transients (as done with trim\_edge=1.0s each side[\[18\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L236-L243)). This explains to users why the segment duration might be slightly shorter than the full plateau. For **amplitude preservation**, since the transient FFT mode already mirrors GWpy’s normalization, it is working as intended. A possible recommendation is adding a test or example demonstrating that a known injection amplitude produces the same ASD peak, confirming the “transient-optimized FFT” yields expected amplitude[\[14\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/_spectral_fourier.py#L208-L217). Additionally, ensure that if any windowing or overlap is used in TimeSeries.asd, it does not unintentionally attenuate the injection tone – but in the code, a half-overlap Hann window (default for ASD) is used uniformly for both injection and background, so the effect cancels out. Overall, the current design effectively balances detection reliability and amplitude fidelity; thus documentation and perhaps a note that \_fft\_transient should be used for short transients (to avoid spectral leakage) would suffice.
## <a name="bootstrap_spectrogram-low-severity"></a>3. bootstrap\_spectrogram – *Low Severity*
**Finding:** The bootstrap spectrogram implementation is comprehensive, including both standard (iid) bootstrap and moving block bootstrap for autocorrelated data. It correctly applies a **Variance Inflation Factor (VIF)** to account for Welch method overlap when using the standard bootstrap. Specifically, after computing the bootstrap distribution of summary statistics (mean or median spectrum) and deriving confidence intervals, it calculates a correction factor via calculate\_correlation\_factor based on the window autocorrelation and number of overlapping segments[\[19\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L470-L478)[\[20\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L480-L488). This factor (>1 for overlapping segments) inflates the error bars (err\_low, err\_high) to reflect reduced effective degrees of freedom[\[21\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L464-L472)[\[22\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L486-L489). When block\_size is specified, the code wisely sets factor = 1.0[\[21\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L464-L472), assuming the block bootstrap inherently captures the time correlation between segments. The **block bootstrap** itself is implemented consistently: it divides the spectrogram time axis into blocks of length block\_size and samples these with replacement to form new surrogate spectrograms[\[23\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L363-L371)[\[24\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L384-L393). The procedure uses moving (possibly overlapping) blocks starting at random positions, and assembles approximately ceil(n\_time/block\_size) blocks to rebuild a full-length sequence[\[23\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L363-L371)[\[25\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L374-L382). Any excess from the last block is truncated to exactly n\_time points[\[26\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L393-L401). This approach (moving block bootstrap) is a standard way to preserve autocorrelation within each resample. The number of bootstrap iterations defaulting to *n\_boot=1000* is a reasonable balance between confidence interval stability and computational load. With 1000 resamples, the standard error of a 68% CI bound is on the order of a few percent, which is acceptable for spectral analysis. The code also leverages Numba to accelerate resampling when available[\[27\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L403-L411), and falls back gracefully if it fails[\[28\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L409-L417). Overall, the methodology is sound: block bootstrap for dependent data and VIF correction for independent bootstrap align with statistical best practices. One minor point: if block\_size is very large (>= n\_time), the code currently just uses one block (with a warning TODO)[\[29\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L352-L360), effectively reverting to no resampling variance. This is an edge case likely left to user discretion.

**Recommendation:** The implementation is correct; recommendations are mostly about **parameter guidance and validation**. Document the rationale that **if data segments are strongly correlated (overlap >0), users should prefer block bootstrap** and choose block\_size roughly equal to the correlation length (in number of spectrogram time-bins). The code sets factor=1 when block\_size>1 (implicitly trusting the bootstrap to handle correlation)[\[21\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L464-L472), which is fine. We suggest adding a warning or note: if block\_size is too small or too large, the error estimates can be under- or over-estimated. For example, if block\_size is 1 (i.e., standard bootstrap), the analytical VIF is applied (already done); if block\_size equals the segment count, perhaps warn that only one block exists, yielding no variance (the code’s pass comment at that condition implies this scenario)[\[29\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L352-L360). Additionally, to justify the number of iterations: include in documentation that *n\_boot=1000* typically yields stable 1-σ CIs, but for high confidence intervals (e.g., 95%) or critical applications, more iterations (e.g., 5000) may be warranted at the cost of computation. Lastly, since the bootstrap returns a FrequencySeries with attached error bands, users should be advised to use those (fs.error\_low, fs.error\_high) for plotting shaded uncertainty. No changes to the core algorithm are needed; it is both numerically and procedurally consistent with theory (as evidenced by the references to Bendat & Piersol in comments) and effectively uses VIF correction[\[19\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L470-L478)[\[22\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L486-L489).
## <a name="xea415bd89bf7b0dcee28fd55de2e2799156f144"></a>4. run\_mcmc (GLS Log-Likelihood & Covariance) – *Medium Severity*
**Finding:** The MCMC implementation correctly formulates the log-likelihood for both ordinary least squares and generalized least squares (GLS) cases. In the FitResult.run\_mcmc method, the log-probability function is defined as -0.5 \* r^T Σ⁻¹ r (plus constants) where *r* is the residual vector[\[30\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L632-L640). If a full covariance matrix is provided (stored as cov\_inv), it computes val = r\*ᶜᵒⁿʲ @ cov\_inv @ r and takes the real part as χ²[\[30\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L632-L640)[\[31\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L634-L642). This is mathematically the correct generalized χ²; the code even notes that for complex residuals (e.g., complex transfer functions), the imaginary parts should cancel out in the Hermitian form[\[31\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L634-L642). In the standard case (no covariance matrix), it falls back to sum of squared residuals divided by diagonal errors (σ), which is equivalent to using Σ⁻¹ = diag(1/σ²)[\[32\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L636-L643). The log-likelihood is thus implemented in a physically correct way for Gaussian errors. The **handling of the covariance matrix** is careful: the GeneralizedLeastSquares fitter (used prior to MCMC) requires an invertible covariance and matching dimensions[\[33\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/gls.py#L115-L123); if cov\_inv is ill-conditioned or singular, the fit routine would likely raise an error or produce large χ². During MCMC, such issues manifest as log\_prob returning -∞ for proposals that cause numerical exceptions (the code catches ValueError/TypeError and returns -inf to reject those samples safely)[\[34\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L644-L652). This provides robustness against any model evaluation errors or matrix issues during sampling. **Numerical stability:** Inversion of covariance is done outside MCMC (presumably when constructing cov\_inv via BifrequencyMap.inverse() or numpy). There is a potential stability concern if the covariance matrix from bootstrap is nearly singular (e.g., if two frequency bins are highly correlated and sample size is small). The MCMC uses whatever cov\_inv is given; if it’s ill-conditioned, r @ cov\_inv @ r could amplify numerical noise. However, since the application (spectral fitting) often involves moderate size matrices (dozens of frequency bins) and the bootstrap covariance is regularized by median/mean statistics, this risk is moderate. The code for initial walker placement is good: it starts walkers in a small Gaussian ball around the best-fit parameters, using either the Minuit errors or a tiny fraction of parameter value[\[35\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L654-L663). This helps the chain converge. The number of walkers (default 32) and steps (5000, with 500 burn-in in high-level API) are sane defaults for ~few-parameter models. One limitation: **complex data with full covariance** is not fully supported – the code in fit\_series raises NotImplemented for complex GLS[\[36\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L54-L61), so MCMC with cov\_inv is effectively used only for real-valued data (like PSDs). That’s fine, just a design note.

**Recommendation:** The implementation is theoretically solid; a recommendation is to ensure the **covariance matrix is well-conditioned** or to provide guidance on regularization. For example, if the bootstrap covariance (BifrequencyMap) is rank-deficient, one could invert it with Tikhonov regularization or drop highly collinear components. It might be useful to warn the user (or automatically regularize) if np.linalg.cond(cov) is very high. Additionally, storing cov\_inv in the FitResult (as done) allows the chain to mark the plot with “GLS fit” for clarity[\[37\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L760-L769). To bolster numerical stability, when building cov\_inv, use np.linalg.solve instead of np.linalg.inv if not already (in gls.py the direct inverse is used if cov provided[\[38\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/gls.py#L46-L54), but for moderate n that’s okay). Another recommendation is to include an example or test for the GLS MCMC: e.g., fit a model to data with known correlation and confirm that the confidence intervals from MCMC (e.g., parameter\_intervals) are tighter with full covariance than assuming diagonal errors, demonstrating the benefit of GLS. Finally, clarify in documentation that **if a covariance matrix is passed, log-likelihood uses the full covariance** and thus the chain’s acceptance probability accounts for frequency correlations. This is an advanced feature, so a brief note on ensuring positive-definiteness of covariance (perhaps advise using the bootstrap covariance output, which is guaranteed symmetric but not explicitly guaranteed PD – although likely PD for a well-behaved average) would be prudent. In summary, just guard against extreme covariance conditioning and document the usage; the core formulation is correct[\[30\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L632-L640).
## <a name="x0c9a82c297f358fa844b6004dbb6ae48720d1fc"></a>5. ARIMA Model Output to TimeSeries – *Low Severity*
**Finding:** The ARIMA integration wraps statsmodels’ results and maps them onto GWpy TimeSeries objects with correct time indexing. The ArimaResult.predict() method accepts either GPS times or index offsets for the start/end of the prediction interval[\[39\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L80-L89). It internally converts a given GPS start time to the corresponding index via \_time\_to\_index (which computes (t\_start - t0)/dt rounded to nearest sample)[\[40\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L70-L78). This ensures that if you specify, say, a start time in GPS seconds, the method will align the output to the closest data point. It then calls statsmodels’ predict() with those indices[\[41\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L96-L104) and creates a TimeSeries for the predicted values. Importantly, it sets the TimeSeries’ start time (t0) to self.t0 + start\_idx\*dt[\[41\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L96-L104). This means the returned prediction series has a GPS timestamp axis consistent with the original data. For example, if the original series started at GPS 1000000000 and we predict from index 50 onward with dt=1s, the prediction’s t0 will be 1000000000 + 50s[\[42\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L100-L108). The **sampling rate (dt)** is preserved exactly in the output TimeSeries (they pass dt=self.dt through)[\[43\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L102-L109), so the cadence of predictions matches the original data. Phase or timing offsets are handled implicitly by using the same dt and aligning to start index; ARIMA does not introduce an inherent phase delay since it’s a forward-prediction model (except for differencing at the very start, which is an initial condition issue). For out-of-sample forecasts, ArimaResult.forecast(steps) obtains the predicted mean and confidence intervals from statsmodels, and then determines the forecast start time as immediately after the last observation: forecast\_t0 = t0 + n\_obs\*dt[\[44\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L139-L147). Here n\_obs is the number of points in the training data; thus if the original series ended at time T\_end, the first forecast point is at T\_end + dt, which is correct[\[45\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L140-L148). It then constructs a TimeSeries for the forecast values with that t0 and dt[\[45\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L140-L148)[\[46\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L147-L155), and similarly TimeSeries for the lower and upper confidence bounds[\[47\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L153-L161). This design cleanly maps the model outputs back to the physical time axis without any gaps or overlaps. **Interpolation:** The ARIMA result does not perform interpolation between the discrete time steps – predictions are inherently at the same sampling rate as the input. If a user requests a prediction start time that is not on an exact sample (e.g., half a sample off), the \_time\_to\_index rounding will choose the nearest index[\[40\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L70-L78). This could introduce at most a 0.5\*dt timing error, but that is acceptable given ARIMA cannot naturally predict “between” sample points. In practice, users will likely specify integral seconds (if dt is 1s) or align with the original sampling grid. No phase delay issues arise, since ARIMA predictions are aligned to the original time grid by design.

**Recommendation:** The current implementation is correct and ensures consistency of time base and sampling rate. One recommendation is to explicitly document how the start and end parameters work – specifically that if you pass GPS times, the function will round to the nearest sample index. This is important for users to know in case their dt is not an integer or if they pass a time slightly inside a sample interval. For clarity, the docstring could note: *“If start (or end) is a GPS timestamp not exactly on the original sample grid, it will be rounded to the nearest sample.”* Another minor suggestion is to handle the case where end time is beyond the data length in predict(): currently end\_idx=None defaults to the end of data (which is fine), but if a user gives a GPS time beyond the last sample, statsmodels may extrapolate (or throw). It might be safer to clamp end\_idx to n\_obs-1 for in-sample prediction, or instruct users to use forecast() for out-of-sample. Additionally, since ARIMA models sometimes have a warm-up period (especially if differencing), the first few predictions might be poor. The code already plots an advisory note in plot() that the very first points may be off[\[48\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L234-L239). This is more of a usage note than a code issue. In summary, the mapping from model output to TimeSeries is done correctly, preserving GPS time references and sampling interval. Just ensure the user guide highlights that forecast() returns the continuation of the TimeSeries (starting at last\_data\_time + dt) and that the returned TimeSeries (or confidence interval series) can be directly overlaid on the original data by time – which it can, because of the matching t0 and dt[\[45\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L140-L148)[\[46\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L147-L155).
## <a name="x181cf53f6910bde2b497186b2d0723e859b4473"></a>6. PCA/ICA on TimeSeriesMatrix – *Medium Severity*
**Finding:** The PCA/ICA routines for TimeSeriesMatrix properly flatten multi-channel time series data and can reconstruct the original data, but with a caveat on shape. The pca\_fit() function treats the input TimeSeriesMatrix of shape (rows, cols, time) by flattening all spatial channels into one “feature” dimension. It reshapes the data to (features, time) and then transposes to (time, features) for scikit-learn[\[49\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L192-L200)[\[50\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L194-L202). Here “features” is essentially the product of (rows × cols) or simply the number of channels if the matrix is 2D (channels × 1 × time). The PCA is fitted on that 2D matrix (time points × features). This flattening is consistent with treating each time sample as an observation (across channels). After fitting, the PCAResult stores channel\_labels from the input matrix (if available)[\[51\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L206-L213) and the time metadata (t0, dt). When transforming data with pca\_transform(), it again flattens the matrix to features and applies model.transform. The **output scores matrix** is then constructed of shape (components, 1, time)[\[52\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L248-L256)[\[53\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L250-L258). Essentially, it creates a new TimeSeriesMatrix where the principal components are treated as “channels” (each PC is a time series) and a dummy second dimension of size 1 is used. The channel names for this matrix are set to “PC1, PC2, …” accordingly[\[54\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L254-L262). This means the dimensionality is reduced from (orig\_features) to (n\_components), as expected. Crucially, the **time index** (t0, dt) is preserved in the new matrix[\[54\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L254-L262), so the principal component time series align exactly with the original data’s time axis (no distortion of temporal structure – PCA is done on synchronous samples, so time ordering remains unchanged). For reconstruction, pca\_inverse\_transform() takes a scores matrix (components × 1 × time) and first flattens it back to (time × components)[\[55\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L289-L297). It then applies inverse\_transform, yielding (time × features) which is transposed to (features × time)[\[56\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L296-L304). At this point, it **does not know the original 2D shape (rows, cols)** because only the flat feature count is retained. The code explicitly comments that the original structure is unknown unless stored, and currently it “assumes (features, 1, samples) as default flat structure” for the reconstructed output[\[57\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L302-L310). Thus, it returns the reconstructed data as a TimeSeriesMatrix of shape (features, 1, time)[\[58\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L300-L308)[\[59\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L313-L321). If the original had been, say, 4 channels of 1 value each (4×1×T), “features” = 4 and this output shape is effectively the same (4×1×T, matching original). If the original was a 2×2 grid (features =4, but originally 2×2×T), the reconstruction will come back as 4×1×T, not 2×2×T. It *does* restore the original channel names if available: after inverse\_transform, it assigns channel\_names = pca\_res.channel\_labels[\[60\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L320-L328). In the 2×2 example, since original likely had no individual channel labels for each cell, channel\_labels might be empty or generic. Essentially, the numeric content of the reconstruction is **correct** (all original feature channels recovered if all components used, or an approximation if reduced components), but the spatial shape is flattened. The **time series structure** (i.e., the temporal order and sampling) is preserved: the returned matrix has the same t0 and dt as input[\[60\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L320-L328), so one can still treat each of the features in the flattened output as a time series. In terms of invertibility, if you use all principal components, the pipeline pca\_transform + pca\_inverse\_transform returns the original data values (with minor floating-point error) – we can see that they apply the inverse scaler to restore original units[\[59\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L313-L321)[\[61\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L315-L319) and there’s no omission of any component if n\_components was not reduced. The only loss is the arrangement of features into multi-dim form.

**Recommendation:** To improve **dimension consistency**, consider storing the original shape (rows, cols) in PCAResult.input\_meta. Currently, input\_meta keeps t0 and dt[\[62\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L208-L213), but adding the spatial shape would enable pca\_inverse\_transform to reshape the flat output back to (rows, cols, time). For example, if matrix.shape = (m, n, t), record orig\_shape=(m,n) and then in inverse transform do X\_rec\_val reshape to (m, n, time) instead of (features, 1, time). This would restore the exact original structure for multi-dimensional fields. Even if this is not implemented, at least document that the reconstructed matrix will be flattened across spatial dimensions. Users dealing with images or 2D sensor arrays should be informed that they may need to reshape the output themselves. On the **time-series preservation** front, the implementation is correct, so just ensure users know that the resulting principal component series can be treated like any other TimeSeriesMatrix (with a single “column”). Perhaps provide a convenience method or example for reconstructing a specific channel’s time series: e.g., show how PC scores can be inverted to get back a particular original channel’s series (which would involve taking all PCs or an appropriate subset). Additionally, one might add an option to pca\_transform to return a TimeSeries object per principal component instead of a matrix – but this is cosmetic. Regarding **ICA**, it likely has similar flattening; ensuring shape handling there would be similarly valuable. In summary, the main improvement is to keep track of original spatial dimensions to perfectly reconstruct the shape. The current approach does not affect numerical correctness or time alignment (which are good), but addressing the dimensional meta-data would enhance “physical consistency” by letting the user obtain, say, the reconstructed multi-channel signal matrix and verify that PCA+inverse yields the original signals (which it will, just in a flattened form). Adding this feature would move the severity to low, as it removes the last ambiguity in an otherwise robust PCA/ICA implementation.

-----
<a name="citations"></a>[\[1\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fields/scalar.py#L538-L546) [\[2\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fields/scalar.py#L540-L548) [\[3\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fields/scalar.py#L284-L292) [\[5\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fields/scalar.py#L548-L555) scalar.py

<https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fields/scalar.py>

[\[4\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/docs/developers/plans/fft_documentation_plan_20260124_1350.md#L40-L45) [\[6\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/docs/developers/plans/fft_documentation_plan_20260124_1350.md#L18-L25) fft\_documentation\_plan\_20260124\_1350.md

<https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/docs/developers/plans/fft_documentation_plan_20260124_1350.md>

[\[7\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L204-L212) [\[8\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L228-L237) [\[9\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L228-L237) [\[10\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L236-L244) [\[11\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L234-L242) [\[12\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L256-L263) [\[13\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L213-L221) [\[17\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L431-L438) [\[18\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py#L236-L243) response.py

<https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/analysis/response.py>

[\[14\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/_spectral_fourier.py#L208-L217) [\[15\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/_spectral_fourier.py#L214-L221) [\[16\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/_spectral_fourier.py#L208-L216) \_spectral\_fourier.py

<https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/_spectral_fourier.py>

[\[19\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L470-L478) [\[20\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L480-L488) [\[21\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L464-L472) [\[22\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L486-L489) [\[23\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L363-L371) [\[24\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L384-L393) [\[25\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L374-L382) [\[26\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L393-L401) [\[27\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L403-L411) [\[28\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L409-L417) [\[29\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py#L352-L360) estimation.py

<https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/spectral/estimation.py>

[\[30\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L632-L640) [\[31\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L634-L642) [\[32\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L636-L643) [\[34\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L644-L652) [\[35\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L654-L663) [\[36\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L54-L61) [\[37\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py#L760-L769) core.py

<https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/core.py>

[\[33\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/gls.py#L115-L123) [\[38\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/gls.py#L46-L54) gls.py

<https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/fitting/gls.py>

[\[39\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L80-L89) [\[40\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L70-L78) [\[41\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L96-L104) [\[42\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L100-L108) [\[43\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L102-L109) [\[44\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L139-L147) [\[45\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L140-L148) [\[46\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L147-L155) [\[47\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L153-L161) [\[48\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py#L234-L239) arima.py

<https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/arima.py>

[\[49\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L192-L200) [\[50\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L194-L202) [\[51\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L206-L213) [\[52\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L248-L256) [\[53\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L250-L258) [\[54\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L254-L262) [\[55\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L289-L297) [\[56\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L296-L304) [\[57\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L302-L310) [\[58\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L300-L308) [\[59\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L313-L321) [\[60\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L320-L328) [\[61\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L315-L319) [\[62\]](https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py#L208-L213) decomposition.py

<https://github.com/tatsuki-washimi/gwexpy/blob/f5c477cf9ae64ad11362a029f8a6055ab523d29c/gwexpy/timeseries/decomposition.py>


Algorithm Validation Report
Frequency-Domain Transformations

Severity: Medium
Finding: Spatial FFT Unit and Sign Convention – The ScalarField.fft_space uses $k = 2\pi,\text{fftfreq}(n, |dx|)$ and flips sign if the original spatial step dx is negative. While this correctly yields angular wavenumber magnitude, the code does not explicitly attach physical units to the resulting $k$-axis. This could lead to confusion in interpretation (e.g. mixing $[\text{rad/m}]$ vs. $[1/\text{m}]$). Additionally, the inverse transform (ifft_space) computes dx_value = 2\pi/(n * |dk|) but doesn’t restore the sign of dx if it was originally negative. This means a field defined on a decreasing spatial grid would come out on an increasing grid after an FFT and inverse FFT, losing the original coordinate orientation.
Recommendation: Propagate units for $k$ (e.g. multiply by 2π and set unit to rad/unit_of_dx) and preserve coordinate direction. For inverse FFT, if the original dx was negative, adjust the reconstructed spatial axis or values accordingly so that a round-trip FFT->IFFT returns the field on the original grid orientation. Clearly document the units of $k$ to avoid misinterpretation.

Severity: Low
Finding: One-Sided Spectrum Edge-Case Scaling – The custom FFT for transients applies a uniform doubling of Fourier amplitudes for all non-zero frequencies to produce a one-sided spectrum. This is a standard approach for amplitude preservation, but it overcorrects the DC bin (and the Nyquist bin, for even-length signals). In the coherence/PSD computation, a similar scaling of 2.0/(fs * window_power) is used indiscriminately, effectively treating DC and Nyquist as if they had mirrored counterparts. In reality, DC (and Nyquist) should not be doubled because they are unique components. This uniform scaling introduces a small bias: the power at DC and Nyquist frequencies is overestimated by a factor of ~2 in the output PSD.
Recommendation: Apply conditional scaling for one-sided spectra – e.g. do not double the 0 Hz component (and the Nyquist frequency if present). This ensures energy is conserved without bias at the edges. The impact is minor unless a significant DC component or Nyquist-frequency signal is present, but correcting it would improve physical fidelity of the PSD amplitude at those frequencies.

Statistical Estimation & Bootstrapping

Severity: Medium
Finding: Correlation in Bootstrapping Not Fully Accounted or Over-corrected – The bootstrap_spectrogram routine addresses overlapped-segment correlation via a Variance Inflation Factor (VIF) formula and optionally through moving block resampling. However, there is a risk of inconsistency in how these are applied. If block_size is not provided, the code uses simple random resampling (assuming independent spectrogram slices) and then likely applies the VIF correction to inflate the confidence intervals. This is appropriate to counter the underestimated variance from overlapping data. If block_size is provided (moving block bootstrap to preserve correlation), the code still computes a VIF and “correction factor”, potentially applying it on top of the block resampling. Stacking both methods would overinflate the error bars, yielding overly conservative intervals. Conversely, if block resampling is meant to replace the analytic VIF, but the code doesn’t disable VIF, it double-counts correlation. In the opposite scenario, if highly correlated data were bootstrapped with naive (block_size=1) sampling and one forgot to apply VIF, the confidence intervals would be too tight (underestimated uncertainty).
Recommendation: Clarify and streamline the correlation handling – use either moving block bootstrap or VIF adjustment, but not both simultaneously on the same data. For example, apply VIF inflation only when using i.i.d. resampling, and skip it when block bootstrap is used (since blocks already capture much of the correlation). Also ensure the VIF formula is used within its valid range (large sample overlap and stationary correlation structure); document that for highly correlated or short records, the formula might under- or over-estimate variance. A consistent approach will improve the reproducibility of spectral error estimates under different bootstrap settings.

Severity: Low
Finding: GLS Matrix Conditioning – The generalized least squares solution β = (Xᵀ W X)⁻¹ Xᵀ W y is solved via np.linalg.solve for stability, which avoids an explicit matrix inverse. However, if the design matrix X has collinear columns or if the covariance matrix Cov = W⁻¹ is ill-conditioned, the normal matrix XᵀWX can be near singular. In such cases, numerical inversion may produce unstable coefficients without warning. This is not a flaw in the math (which is correct) but a risk in edge cases – e.g. fitting strongly correlated predictors or using a poorly estimated covariance.
Recommendation: Monitor the condition number of XᵀWX. If it’s large (ill-conditioned), consider regularization or switch to a solver that can handle rank deficiency (like SVD/Pseudo-inverse or ridge regression). Although this primarily affects extreme cases, adding a condition check can alert the user to potential reliability issues in the fitted parameters. Ensuring cov_inv is positive-definite (or using a small Tikhonov regularization for nearly singular covariances) would safeguard the numerical integrity of the GLS results.

Time-Series Modeling & Transformations

Severity: Medium
Finding: Step Detection Sensitivity – The automatic step segmentation in ResponseFunctionAnalysis.detect_step_segments may misidentify boundaries under certain conditions. The algorithm breaks an injection timeline whenever the dominant frequency changes beyond a tolerance or the SNR falls below threshold. If the injection frequency wanders slightly (e.g. due to noise) or the SNR dips momentarily, the logic could falsely trigger a new “step.” This would fragment what is physically a single constant-frequency interval. Conversely, if two close frequency steps occur with only a small change, the code might fail to split them if the difference is below freq_tolerance. Such mis-segmentation can impact the downstream coupling function calculation – e.g., combining data from two different injection frequencies or cutting an injection in half would both yield incorrect amplitude estimates.
Recommendation: Increase the robustness of step detection. Possible improvements include smoothing the tracked frequency series before comparison, requiring a minimum duration for a new segment (to ignore blips), or using a hysteresis in threshold (so a small dip in SNR doesn’t immediately end a segment). Logging diagnostic info (e.g. when and why a cut is made) can help users verify that detected segments align with true injection changes. These tweaks would ensure the response function is computed on truly steady-frequency segments, preserving physical validity of the transfer function estimates.

Severity: Medium
Finding: Time Axis Gaps in DTT TimeSeries Parsing – The dttxml.parse_timeseries function does not fully handle time-axis metadata for certain data subtypes. For “(t, Y)” formatted time series (subtypes 4, 5, 6 in DTT XML), the code decodes the entire data array but never separates the time values from the signal – it simply labels the subtype and returns the raw data in timebunch.data_raw. No TimeSeries object is constructed for these cases. This means the timestamp information is effectively ignored (the time array is present in the data but not used to build a proper time axis in the result). Additionally, for downsampled or filtered time series, the XML provides fields like DecimationDelay and TimeDelay (filter group delay), but the parser only stores these in the output structure without adjusting the timing. The returned timebunch.timeseries for normal subtypes uses the start time (t0) and sample interval (dt) directly, not accounting for an initial delay. In a down-converted series, this means the first sample in the array actually corresponds to t0 + time_delay, but the code still treats it as t0.
Recommendation: Implement proper handling for time-indexed series and filter delays. For subtypes that include an explicit time array, split the array into a time vector and data vector, and use them to instantiate a TimeSeries (or at least provide them separately in the Bunch). For decimated data with known delay, adjust the effective start time: e.g., set timebunch.t0_corrected = gps_second + time_delay_s and perhaps issue a warning that the data is phase-delayed by the filter. Without these adjustments, users may unknowingly misalign datasets in time, which is critical in physical analyses (e.g., aligning injection measurements or coincident signals). Document these fields so that users can manually correct if needed, until the parser applies the offsets automatically.

Dimensionality Reduction (PCA/ICA)

Severity: Medium
Finding: Incomplete Shape Restoration in PCA Inverse – The PCA implementation flattens multi-dimensional time-series data but does not fully unflatten it on reconstruction. Specifically, pca_fit flattens a TimeSeriesMatrix of shape (channels, cols, time) into a 2D array of shape (features, time) (with features = channels × cols) for PCA. The forward transform (pca_transform) outputs principal components as a matrix of shape (components, 1, time). However, the inverse transform currently assumes all original features belonged to a single combined axis. It returns an array of shape (features, 1, time), effectively treating all original sub-channels as one channel axis. Any original distinction between “channels” and “columns” is lost – for example, if you had 5 channels of 3 columns each, the inverse will return 15 channels of 1 column. This can lead to misinterpretation of the reconstructed data (mixing what were separate spatial/field dimensions into one axis). The metadata (channel names) is preserved only for the first axis, so if multiple column sets existed, their identity is not restored.
Recommendation: Track the original shape and axes during PCA fitting. A simple fix is to store the original (n_channels, n_cols) dimensions in the PCAResult (e.g. in input_meta). The pca_inverse_transform can then reshape the recovered data back to (n_channels, n_cols, time) instead of (features, 1, time). This would preserve the physical structure of the data. In the interim, users should be cautious to reshape the output manually if they know the original layout. Clearly documenting that the current inverse returns a flattened structure (and that channel_names will correspond to the first dimension only) will prevent mis-interpretation of results in multi-dimensional cases.

File Parsing & Transfer Functions

Severity: High
Finding: Loss of Phase Information in Transfer Functions – The dttxml.parse_transfer routine appears to ignore the imaginary component of complex transfer functions, which is a critical error. In subtypes where transfer functions are stored as complex numbers, the code reads the binary stream and then takes only the real part for processing. For example, subtype 3 (“transfer function B/A in format (f, Y)”) reads the entire dataset as complex64 and then uses .real to interpret the first N points as frequencies, implicitly assuming the imaginary parts of those frequency entries are zero (which they likely are). It then assigns the remaining data to xfer but also only retains the real part in subtype 6. This means any imaginary component of the transfer function (phase information) would be dropped. A transfer function is generally complex (with both magnitude and phase); dropping the imaginary part essentially yields only the real part of the frequency response, which is unphysical unless the phase happens to be zero. The algorithm as written would silently produce an incorrect transfer function if the data had non-zero phase.
Recommendation: Preserve complex data throughout transfer function parsing. The parser should use the appropriate complex dtype (e.g. complex128 if the data are double precision real & imaginary, or ensure the imaginary part is captured for single precision complex). For subtype formats that intermix frequency and complex response, a safer approach is: read the first N values as floats (frequency axis) and the next N*M values as complex numbers for the transfer function. Under no circumstance should the imaginary part of the transfer function be discarded. If the DTT XML uses a format where the imaginary parts are stored implicitly or separately, the parser must be updated to reconstruct the full complex number. This fix is critical, as phase information is often as important as magnitude for physical interpretation of transfer functions.

Severity: Medium
Finding: DTT→SciPy PSD Conversion Factor – The function convert_scipy_to_dtt is intended to convert a PSD normalized in the SciPy/Welch convention to LIGO’s DTT convention. However, the implementation does not actually use the is_one_sided parameter, and the formula applied is static. In Welch’s convention, a one-sided PSD for real signals is scaled such that the positive frequency bins are doubled (except DC/Nyquist) and the PSD units are per Hz. DTT’s convention differs by a factor of $N/ \sum w$² (for window $w$). The code computes ratio = (sum_w2 * N) / (sum_w**2), which is correct for converting a one-sided SciPy PSD to DTT. But if is_one_sided=False (i.e. a two-sided PSD input), the same ratio is used; in that case, the output would be off by a factor of 2. Essentially, the code ignores whether the input PSD already accounted for the one-sided doubling. This can lead to a ~2× error in PSD amplitude when converting spectra that are not in the assumed format.
Recommendation: Honor the is_one_sided flag. If the input PSD is one-sided (typical for real time series in SciPy), the current conversion ratio is fine. If the input is two-sided, the conversion should first multiply the ratio by 0.5 (or otherwise adjust) to avoid an over-scaling. Implementing this conditional or requiring the user to specify the spectrum type will prevent mis-scaled PSDs. It’s also good to explicitly document the expected input: e.g., “Assumes one-sided PSD; if two-sided, set is_one_sided=False to halve the output”. This ensures consistency with DTT’s definitions and avoids confusion in amplitude calibrations when comparing outputs between SciPy and DTT conventions.